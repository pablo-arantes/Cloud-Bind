{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablo-arantes/Cloud-Bind/blob/main/Virtual_Screening.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj2BWZxUDbEE"
      },
      "source": [
        "# **Hi there!**\n",
        "\n",
        "This is a Jupyter notebook for running a Virtual Screening protocol, including molecular docking calculations with deep learning using the Gnina docking software, the Protein-Ligand Atomistic Conformational Ensemble Resolver and the AEV-PLIG binding affinity predictor.\n",
        "\n",
        "The main goal of this notebook is to demonstrate how to harness the power of cloud-computing to perform drug binding structure prediction in a cheap and yet feasible fashion.\n",
        "\n",
        "---\n",
        "\n",
        " **This notebook is NOT a standard protocol for molecular docking calculations!** It is just a simple docking pipeline illustrating each step of the process.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Bugs**\n",
        "- If you encounter any bugs, please report the issue to https://github.com/pablo-arantes/Cloud-Bind/issues\n",
        "\n",
        "**Acknowledgments**\n",
        "- We would like to thank the [GNINA](https://github.com/gnina/gnina) team for doing an excellent job open sourcing the software.\n",
        "- We would like to thank the [Roitberg](https://roitberg.chem.ufl.edu/) team for developing the fantastic [TorchANI](https://github.com/aiqm/torchani).\n",
        "- We would like to thank [@ruiz_moreno_aj](https://twitter.com/ruiz_moreno_aj) for his work on [Jupyter Dock](https://github.com/AngelRuizMoreno/Jupyter_Dock)\n",
        "- We would like to thank the ChemosimLab ([@ChemosimLab](https://twitter.com/ChemosimLab)) team for their incredible [ProLIF](https://prolif.readthedocs.io/en/latest/index.html#) (Protein-Ligand Interaction Fingerprints) tool.\n",
        "- We would like to thank the [OpenBPMD](https://github.com/Gervasiolab/OpenBPMD) team for their open source implementation of binding pose metadynamics (BPMD).\n",
        "- Also, credit to [David Koes](https://github.com/dkoes) for his awesome [py3Dmol](https://3dmol.csb.pitt.edu/) plugin.\n",
        "- Finally, we would like to thank [Making it rain](https://github.com/pablo-arantes/making-it-rain) team, **Pablo R. Arantes** ([@pablitoarantes](https://twitter.com/pablitoarantes)), **Marcelo D. PolÃªto** ([@mdpoleto](https://twitter.com/mdpoleto)), **Conrado Pedebos** ([@ConradoPedebos](https://twitter.com/ConradoPedebos)) and **Rodrigo Ligabue-Braun** ([@ligabue_braun](https://twitter.com/ligabue_braun)), for their amazing work.\n",
        "- For related notebooks see: [Cloud-Bind](https://github.com/pablo-arantes/Cloud-Bind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5xm80qcWGWd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Install Conda Colab**\n",
        "#@markdown It will restart the kernel (session), don't worry.\n",
        "# !pip install -q condacolab\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://github.com/conda-forge/miniforge/releases/download/25.3.1-0/Miniforge3-Linux-x86_64.sh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPLn-HhIAiQF"
      },
      "outputs": [],
      "source": [
        "import condacolab\n",
        "condacolab.check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH1oMiVUlxO_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Install dependencies**\n",
        "#@markdown It will take a few minutes, please, have a coffee and wait. ;-)\n",
        "# install dependencies\n",
        "%%capture\n",
        "import sys\n",
        "import tarfile\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "#subprocess.run(\"rm -rf /usr/local/conda-meta/pinned\", shell=True)\n",
        "subprocess.run(\"pip -q install py3Dmol\", shell=True)\n",
        "subprocess.run(\"pip install git+https://github.com/pablo-arantes/biopandas\", shell=True)\n",
        "subprocess.run(\"pip install bio\", shell=True)\n",
        "subprocess.run(\"pip install torch torchvision\", shell=True)\n",
        "subprocess.run(\"pip install torchani\", shell=True)\n",
        "subprocess.run(\"pip install ase\", shell=True)\n",
        "subprocess.run(\"pip install pandas\", shell=True)\n",
        "subprocess.run(\"pip install seaborn\", shell=True)\n",
        "subprocess.run(\"pip install openmm\", shell=True)\n",
        "subprocess.run(\"pip install datamol\", shell=True)\n",
        "subprocess.run(\"conda install -c conda-forge pdbfixer -y\", shell=True)\n",
        "subprocess.run(\"pip install parmed\", shell=True)\n",
        "subprocess.run(\"conda install -c conda-forge openbabel -y\", shell=True)\n",
        "subprocess.run(\"pip install rdkit\", shell=True)\n",
        "subprocess.run(\"wget https://github.com/gnina/gnina/releases/download/v1.3/gnina\", shell=True)\n",
        "subprocess.run(\"chmod +x gnina\", shell=True)\n",
        "subprocess.run(\"git clone https://github.com/pablo-arantes/AEV-PLIG.git\", shell=True)\n",
        "subprocess.run(\"pip install logmd==0.1.30\", shell=True)\n",
        "subprocess.run(\"pip install MDAnalysis\", shell=True)\n",
        "subprocess.run(\"pip install posebusters --upgrade\", shell=True)\n",
        "subprocess.run(\"mamba install -c conda-forge pymol-open-source -y\", shell=True)\n",
        "subprocess.run(\"pip install qcelemental\", shell=True)\n",
        "subprocess.run(\"pip install torch-geometric\", shell=True)\n",
        "subprocess.run(\"wget https://github.com/rdk/p2rank/releases/download/2.5/p2rank_2.5.tar.gz\", shell=True)\n",
        "file = tarfile.open('p2rank_2.5.tar.gz')\n",
        "file.extractall('/content/')\n",
        "file.close()\n",
        "os.remove('p2rank_2.5.tar.gz')\n",
        "subprocess.run(\"pip install medchem\", shell=True)\n",
        "\n",
        "#load dependencies\n",
        "import parmed as pmd\n",
        "from biopandas.pdb import PandasPdb\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import py3Dmol\n",
        "import platform\n",
        "import scipy.cluster.hierarchy\n",
        "from scipy.spatial.distance import squareform\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.interpolate import griddata\n",
        "import seaborn as sb\n",
        "from statistics import mean, stdev\n",
        "from matplotlib import colors\n",
        "from IPython.display import set_matplotlib_formats\n",
        "from rdkit import Chem\n",
        "import datamol as dm\n",
        "import seaborn as sns\n",
        "from concurrent.futures import ProcessPoolExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1xqOvPEs2y5z"
      },
      "outputs": [],
      "source": [
        "#@title **Install PLACER dependencies**\n",
        "#@markdown Please, continue drinking your coffee and wait. ;-)\n",
        "\n",
        "#@markdown Protein-Ligand Atomistic Conformational Ensemble Resolver (PLACER) is a graph neural network that operates entirely at the atomic level; the nodes of the graph are the atoms in the system. PLACER was trained to recapitulate the correct atom positions from partially corrupted input structures from the Cambridge Structural Database and the Protein Data Bank. PLACER accurately generates structures of diverse organic small molecules given knowledge of their atom composition and bonding, and given a description of the larger protein context, can accurately build up structures of small molecules and protein side chains; used in this way PLACER has competitive performance on protein-small molecule docking given approximate knowledge of the binding site. PLACER is a rapid and stochastic denoising network, which enables generation of ensembles of solutions to model conformational heterogeneity.\n",
        "\n",
        "#@markdown Reference: https://www.biorxiv.org/content/10.1101/2024.09.25.614868v1\n",
        "\n",
        "\n",
        "\n",
        "#install dependencies\n",
        "%%capture\n",
        "import sys\n",
        "import tarfile\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "commands = [\n",
        "    \"git clone https://github.com/pablo-arantes/PLACER.git\",\n",
        "    \"mamba env create -f /content/PLACER/envs/placer_env_lite.yml\"\n",
        "]\n",
        "\n",
        "\n",
        "for cmd in commands:\n",
        "    subprocess.run(cmd, shell=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDQnAKJLFxtt"
      },
      "source": [
        "## Using Google Drive to store simulation data\n",
        "\n",
        "Google Colab does not allow users to keep data on their computing nodes. However, we can use Google Drive to read, write, and store our simulations files. Therefore, we suggest to you to:\n",
        "\n",
        "1.   Create a folder in your own Google Drive and copy the necessary input files there.\n",
        "2.   Copy the path of your created directory. We will use it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Lm7Akepv_vl-"
      },
      "outputs": [],
      "source": [
        "#@title ### **Import Google Drive**\n",
        "#@markdown Click the \"Run\" buttom to make your Google Drive accessible.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n96Vyb3mMemN"
      },
      "outputs": [],
      "source": [
        "#@title **Check if you correctly allocated GPU nodes**\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZwl66HTGI7v"
      },
      "source": [
        "\n",
        "---\n",
        "# **Loading the necessary input files**\n",
        "\n",
        "At this point, we should have all libraries and dependencies installed.\n",
        "\n",
        "**Important**: Make sure the PDB file points to the correct structure.\n",
        "\n",
        "Below, you should provide the names of all input files and the pathway of your Google Drive folder containing them.\n",
        "\n",
        "**Please, don't use spaces in the files and folders names, i.e. MyDrive/protein_ligand and so on.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Ri7rCQdMUS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Please, provide the necessary input files below for receptor**:\n",
        "#@markdown **Important:** Run the cell to prepare your receptor and select your reference residue for the construction of an optimal box size for the docking calculations.\n",
        "\n",
        "#@markdown Choose between uploading your own PDB file (pdb_file) or using a PDB ID (Query_PDB_ID) to download the correct file. The appropriate chains can be selected as well.\n",
        "\n",
        "from openmm.app.pdbfile import PDBFile\n",
        "\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "from Bio.PDB import PDBParser, PDBIO, Select\n",
        "from Bio.PDB import is_aa\n",
        "import pandas as pd\n",
        "from pdbfixer import PDBFixer\n",
        "\n",
        "Google_Drive_Path = '/content/drive/MyDrive/' #@param {type:\"string\"}\n",
        "workDir = Google_Drive_Path\n",
        "\n",
        "workDir2 = os.path.join(workDir)\n",
        "workDir_check = os.path.exists(workDir2)\n",
        "if workDir_check == False:\n",
        "  os.mkdir(workDir2)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "if os.path.exists(os.path.join(workDir, \"name_residues.txt\")):\n",
        "  os.remove(os.path.join(workDir, \"name_residues.txt\"))\n",
        "  os.remove(os.path.join(workDir,\"name_residues_receptor.txt\"))\n",
        "else:\n",
        "  pass\n",
        "\n",
        "temp = os.path.join(workDir, \"temp.pdb\")\n",
        "receptor = os.path.join(workDir, \"receptor.pdb\")\n",
        "ligand = os.path.join(workDir, \"ligand.sdf\")\n",
        "\n",
        "# Choose PDB source: Upload or PDB ID\n",
        "PDB_Source = \"Uploaded_PDB\" # @param [\"PDB_ID\",\"Uploaded_PDB\"]\n",
        "\n",
        "if PDB_Source == \"Uploaded_PDB\":\n",
        "    pdb_file = 'protein.pdb' #@param {type:\"string\"}\n",
        "    outfnm = os.path.join(workDir, pdb_file)\n",
        "\n",
        "elif PDB_Source == \"PDB_ID\":\n",
        "    Query_PDB_ID = '9BDQ' #@param {type:\"string\"}\n",
        "    pdbfn = Query_PDB_ID + \".pdb\"\n",
        "    url = 'https://files.rcsb.org/download/' + pdbfn\n",
        "    outfnm = os.path.join(workDir, pdbfn)\n",
        "    try:\n",
        "      response = requests.get(url)\n",
        "      response.raise_for_status()  # Raise an exception for bad responses (4xx or 5xx)\n",
        "      with open(outfnm, 'wb') as outfile:\n",
        "          outfile.write(response.content)\n",
        "      print(f\"File downloaded to: {outfnm}\")\n",
        "      print(f\"File size: {os.path.getsize(outfnm)} bytes\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Error downloading PDB file: {e}\")\n",
        "\n",
        "print(outfnm)\n",
        "# Read the PDB file\n",
        "ppdb = PandasPdb().read_pdb(outfnm)\n",
        "selected_chains = ['A'] # @param {\"type\":\"raw\"}\n",
        "\n",
        "# Check if selected_chains is empty or not provided:\n",
        "if 'selected_chains' not in locals() or not selected_chains:\n",
        "    print(\"No chains selected, processing all chains.\")\n",
        "    # Filter chains for ATOM and HETATM records\n",
        "    ppdb.df['ATOM'] = ppdb.df['ATOM']\n",
        "else:\n",
        "    ppdb.df['ATOM'] = ppdb.df['ATOM'][ppdb.df['ATOM']['chain_id'].isin(selected_chains)]\n",
        "    ppdb.df['HETATM'] = ppdb.df['HETATM'][ppdb.df['HETATM']['chain_id'].isin(selected_chains)]\n",
        "\n",
        "# Remove water molecules (HOH)\n",
        "ppdb.df['HETATM'] = ppdb.df['HETATM'][ppdb.df['HETATM']['residue_name'] != 'HOH']\n",
        "\n",
        "# Save the filtered PDB file\n",
        "ppdb.to_pdb(path=temp, records=['ATOM', 'HETATM'], gz=False, append_newline=True)\n",
        "\n",
        "# Prepare receptor (additional filtering)\n",
        "ppdb = PandasPdb().read_pdb(outfnm)\n",
        "\n",
        "# Remove water molecules (HOH)\n",
        "ppdb.df['HETATM'] = ppdb.df['HETATM'][ppdb.df['HETATM']['residue_name'] != 'HOH']\n",
        "\n",
        "# Remove OXT atoms and hydrogen atoms\n",
        "ppdb.df['ATOM'] = ppdb.df['ATOM'][ppdb.df['ATOM']['atom_name'] != 'OXT']\n",
        "ppdb.df['ATOM'] = ppdb.df['ATOM'][ppdb.df['ATOM']['element_symbol'] != 'H']\n",
        "\n",
        "# Save the filtered receptor PDB file\n",
        "ppdb.to_pdb(path=receptor, records=['ATOM', 'HETATM'], gz=False, append_newline=True)\n",
        "\n",
        "fixer = PDBFixer(filename=receptor)\n",
        "fixer.removeHeterogens()\n",
        "fixer.findMissingResidues()\n",
        "fixer.findMissingAtoms()\n",
        "fixer.addMissingAtoms()\n",
        "fixer.addMissingHydrogens(pH=7.4)\n",
        "PDBFile.writeFile(fixer.topology, fixer.positions, open(receptor, 'w'))\n",
        "\n",
        "\n",
        "path = '/content/'\n",
        "\n",
        "\n",
        "def is_het(residue):\n",
        "    res = residue.id[0]\n",
        "    return res != \" \" and res != \"W\"\n",
        "\n",
        "def aa(residue):\n",
        "    res = residue.id[0]\n",
        "    return res != \"W\"\n",
        "\n",
        "\n",
        "class ResidueSelect(Select):\n",
        "    def __init__(self, chain, residue):\n",
        "        self.chain = chain\n",
        "        self.residue = residue\n",
        "\n",
        "    def accept_chain(self, chain):\n",
        "        return chain.id == self.chain.id\n",
        "\n",
        "    def accept_residue(self, residue):\n",
        "        return residue == self.residue and aa(residue)\n",
        "\n",
        "def extract_ligands(path):\n",
        "    pdb = PDBParser().get_structure(temp, temp)\n",
        "    io = PDBIO()\n",
        "    io.set_structure(pdb)\n",
        "    i = 1\n",
        "    name_residues = []\n",
        "    for model in pdb:\n",
        "      for chain in model:\n",
        "        for residue in chain:\n",
        "          if not aa(residue):\n",
        "            continue\n",
        "          # print(f\"{chain[i].resname} {i}\")\n",
        "          name_residues.append(residue)\n",
        "          print((f\"saving {residue}\"), file=open(os.path.join(workDir, \"name_residues.txt\"), \"a\",))\n",
        "          i += 1\n",
        "\n",
        "extract_ligands(path)\n",
        "\n",
        "def extract_ligands2(path):\n",
        "    pdb = PDBParser().get_structure(receptor, receptor)\n",
        "    io = PDBIO()\n",
        "    io.set_structure(pdb)\n",
        "    i2 = 1\n",
        "    name_residues2 = []\n",
        "    for model in pdb:\n",
        "      for chain in model:\n",
        "        for residue in chain:\n",
        "          if not aa(residue):\n",
        "            continue\n",
        "          # print(f\"{chain[i].resname} {i}\")\n",
        "          name_residues2.append(residue)\n",
        "          print((f\"saving {residue}\"), file=open(os.path.join(workDir, \"name_residues_receptor.txt\"), \"a\",))\n",
        "          i2 += 1\n",
        "\n",
        "extract_ligands2(path)\n",
        "\n",
        "\n",
        "dataset = pd.read_csv(os.path.join(workDir, 'name_residues.txt'), delimiter = \" \", header=None)\n",
        "df = pd.DataFrame(dataset)\n",
        "df = df.iloc[:, [2]]\n",
        "new = df.to_numpy()\n",
        "\n",
        "dataset2 = pd.read_csv(os.path.join(workDir, 'name_residues_receptor.txt'), delimiter = \" \", header=None)\n",
        "df2 = pd.DataFrame(dataset2)\n",
        "df2 = df2.iloc[:, [2]]\n",
        "new2 = df2.to_numpy()\n",
        "\n",
        "b = 1\n",
        "res_number = []\n",
        "for j in new2:\n",
        "  res_number.append(b)\n",
        "  b += 1\n",
        "\n",
        "print(\"Residue\" + \" - \"  + \"Number\" )\n",
        "a = 1\n",
        "for j in new:\n",
        "  print(', '.join(j) + \" - \"  + str(a))\n",
        "  a += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zS9eKySxq13",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Predict ligand-binding pockets from your protein structure using P2Rank**:\n",
        "#@markdown **P2Rank** is a stand-alone command line program that predicts ligand-binding pockets from a protein structure. It achieves high prediction success rates without relying on an external software for computation of complex features or on a database of known protein-ligand templates.\n",
        "#@markdown P2Rank makes predictions by scoring and clustering points on the protein's solvent accessible surface. Ligandability score of individual points is determined by a machine learning based model trained on the dataset of known protein-ligand complexes. For more details see [here](https://github.com/rdk/p2rank).\n",
        "\n",
        "import subprocess\n",
        "import csv\n",
        "import os\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
        "!update-alternatives --set java /usr/lib/jvm/java-17-openjdk-amd64/bin/java\n",
        "!update-alternatives --set javac /usr/lib/jvm/java-17-openjdk-amd64/bin/javac\n",
        "\n",
        "print(f\"Receptor file: {receptor}\")\n",
        "output_p2rank = os.path.join(workDir, \"output_p2rank\")\n",
        "print(f\"P2Rank output directory: {output_p2rank}\")\n",
        "\n",
        "p2rank = \"/content/p2rank_2.5/prank predict -f \" + str(receptor) + \" -o \" + str(output_p2rank)\n",
        "print(f\"P2Rank command: {p2rank}\")\n",
        "\n",
        "original_stdout = sys.stdout\n",
        "with open('p2rank.sh', 'w') as f:\n",
        "  sys.stdout = f\n",
        "  print(p2rank)\n",
        "  sys.stdout = original_stdout\n",
        "subprocess.run([\"chmod 700 p2rank.sh\"], shell=True)\n",
        "#subprocess.run([\"./p2rank.sh\"], shell=True,)\n",
        "result = subprocess.run([\"./p2rank.sh\"], shell=True, capture_output=True, text=True)\n",
        "print(\"P2Rank stdout:\", result.stdout)\n",
        "print(\"P2Rank stderr:\", result.stderr)\n",
        "\n",
        "\n",
        "# Check if the output file exists\n",
        "output_file = os.path.join(workDir, \"output_p2rank/receptor.pdb_predictions.csv\")\n",
        "if os.path.exists(output_file):\n",
        "  with open(output_file, 'r') as file:\n",
        "    csvreader = csv.reader(file)\n",
        "    residue = []\n",
        "    score = []\n",
        "    center_x = []\n",
        "    center_y = []\n",
        "    center_z = []\n",
        "    for row in csvreader:\n",
        "      residue.append(row[9:10])\n",
        "      score.append(row[2:3])\n",
        "      center_x.append(row[6:7])\n",
        "      center_y.append(row[7:8])\n",
        "      center_z.append(row[8:9])\n",
        "\n",
        "  for i in range(1,len(residue)):\n",
        "    file = str((residue[i])[0]).split()\n",
        "    score_end = str((score[i])[0]).split()\n",
        "    center_x_end = str((center_x[i])[0]).split()\n",
        "    center_y_end = str((center_y[i])[0]).split()\n",
        "    center_z_end = str((center_z[i])[0]).split()\n",
        "    print(\"Pocket \" + str(i))\n",
        "    print(\"Score = \" + score_end[0])\n",
        "    final_residues = []\n",
        "    for i in range(0,len(file)):\n",
        "      test = file[i]\n",
        "      final_residues.append(int(test[2:]))\n",
        "    print(\"Selected Residues = \" + str(final_residues))\n",
        "    print(\"Center x = \"+ str(center_x_end[0]), \"Center y = \"+ str(center_y_end[0]), \"Center z = \"+ str(center_z_end[0]) + \"\\n\")\n",
        "else:\n",
        "  print(f\"Error: P2Rank output file not found at: {output_file}\")\n",
        "  print(\"Please, check if P2Rank executed successfully and generated the expected output.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8knPKMYiNcYE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Please, provide the pocket or residue number for the selection**:\n",
        "#@markdown **Important:** The selected pocket or residues will be used as a reference for the construction of an optimal box size for the ligand during the docking. If you want to select more than one residue, please, use comma to separte the numbers (i.e. 147,150,155,160). **Please, DO NOT USE SPACES BETWEEN THEM.**\n",
        "\n",
        "\n",
        "import re\n",
        "import csv\n",
        "\n",
        "if os.path.exists(os.path.join(workDir, \"name_residue.txt\")):\n",
        "  os.remove(os.path.join(workDir, \"name_residue.txt\"))\n",
        "else:\n",
        "  pass\n",
        "\n",
        "# Python code to convert string to list\n",
        "def Convert(string):\n",
        "\tli = list(string.split(\",\"))\n",
        "\treturn li\n",
        "\n",
        "def extract_ligands(path,residues):\n",
        "    pdb = PDBParser().get_structure(temp, temp)\n",
        "    io = PDBIO()\n",
        "    io.set_structure(pdb)\n",
        "    i = 1\n",
        "    name_residues = []\n",
        "    for model in pdb:\n",
        "      for chain in model:\n",
        "        for residue in chain:\n",
        "          if not aa(residue):\n",
        "            continue\n",
        "          if i == int(residues):\n",
        "            # print(residues)\n",
        "            print((f\"saving {residue}\"), file=open(os.path.join(workDir, \"name_residue.txt\"), \"a\",))\n",
        "            io.save(f\"res_{i}_certo.pdb\", ResidueSelect(chain, residue))\n",
        "          i += 1\n",
        "\n",
        "Selection = \"Pocket\" #@param [\"Pocket\", \"Residues\"]\n",
        "\n",
        "number = '1' #@param {type:\"string\"}\n",
        "\n",
        "if Selection == \"Pocket\":\n",
        "  file = str((residue[int(number)])[0]).split()\n",
        "  score_end = str((score[int(number)])[0]).split()\n",
        "  center_x_end = str((center_x[int(number)])[0]).split()\n",
        "  center_y_end = str((center_y[int(number)])[0]).split()\n",
        "  center_z_end = str((center_z[int(number)])[0]).split()\n",
        "  center_x_gnina = float(center_x_end[0])\n",
        "  center_y_gnina = float(center_y_end[0])\n",
        "  center_z_gnina = float(center_z_end[0])\n",
        "  print(\"Pocket \" + str(number))\n",
        "  print(\"Score = \" + score_end[0])\n",
        "  print(\"Center x = \"+ str(center_x_end[0]), \"Center y = \"+ str(center_y_end[0]), \"Center z = \"+ str(center_z_end[0]) + \"\\n\")\n",
        "  final_residues = []\n",
        "  for i in range(0,len(file)):\n",
        "    test = file[i]\n",
        "    final_residues.append(int(test[2:]))\n",
        "  residues_num = final_residues\n",
        "else:\n",
        "  residues_num = Convert(number)\n",
        "\n",
        "filenames=[]\n",
        "for k in range(0, len(residues_num)):\n",
        "  extract_ligands(path, residues_num[k])\n",
        "  filenames.append(f\"res_{residues_num[k]}_certo.pdb\")\n",
        "\n",
        "\n",
        "with open('selection_merge.pdb', 'w') as outfile:\n",
        "    for fname in filenames:\n",
        "        with open(fname) as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)\n",
        "\n",
        "# reading each line from original\n",
        "# text file\n",
        "file1 = open('/content/selection_merge.pdb', 'r')\n",
        "file2 = open('/content/selection_merge_end.pdb','w')\n",
        "\n",
        "for line in file1.readlines():\n",
        "\n",
        "    # reading all lines that begin\n",
        "    # with \"TextGenerator\"\n",
        "    x = re.findall(\"^END\", line)\n",
        "\n",
        "    if not x:\n",
        "        file2.write(line)\n",
        "\n",
        "# close and save the files\n",
        "file1.close()\n",
        "file2.close()\n",
        "\n",
        "dataset = pd.read_csv(os.path.join(workDir, \"name_residue.txt\"), delimiter = \" \", header=None)\n",
        "df = pd.DataFrame(dataset)\n",
        "df = df.iloc[:, [2]]\n",
        "new = df.to_numpy()\n",
        "\n",
        "print(\"Selected Residue\" + \" - \"  + \"Number\" )\n",
        "for j, i in zip(new, range(0, len(residues_num))):\n",
        "# for j in new:\n",
        "  print(', '.join(j) + \" - \"  + str(residues_num[i]))\n",
        "res_box = '/content/selection_merge_end.pdb'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ak4T6zB-R-w4"
      },
      "outputs": [],
      "source": [
        "#@title **Receptor Visualization**:\n",
        "#@markdown Now that the protein has been sanitized and the selection has been chosen, it is a good idea to visualize and check the protein (gray) and your selection (green).\n",
        "\n",
        "view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
        "view.removeAllModels()\n",
        "view.setViewStyle({'style':'outline','color':'black','width':0.1})\n",
        "\n",
        "view.addModel(open(receptor,'r').read(),format='pdb')\n",
        "Prot=view.getModel()\n",
        "Prot.setStyle({'cartoon':{'arrows':True, 'tubes':True, 'style':'oval', 'color':'white'}})\n",
        "view.addSurface(py3Dmol.VDW,{'opacity':0.6,'color':'white'})\n",
        "\n",
        "\n",
        "view.addModel(open(res_box,'r').read(),format='mol2')\n",
        "ref_m = view.getModel()\n",
        "ref_m.setStyle({},{'stick':{'colorscheme':'greenCarbon','radius':0.2}})\n",
        "\n",
        "view.zoomTo()\n",
        "view.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLwXCHI30fqz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Please, provide the necessary input files for the ligand**:\n",
        "\n",
        "#@markdown Type the smiles or a filename (SMI, CSV or SDF format) of your molecule. **Ex: C=CC(=O)OC, molecules.smi, molecules.csv or molecules.sdf**\n",
        "\n",
        "#@markdown Just remember that if you want to use a smi, a csv or a sdf file, you should first upload the file here in Colab or in your Google Drive, and then provide the path for the file.\n",
        "\n",
        "#@markdown If you don't know the exact smiles for your molecule, please, check https://pubchem.ncbi.nlm.nih.gov/\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import rdMolTransforms\n",
        "from rdkit.Chem.Draw import rdMolDraw2D\n",
        "from rdkit.Chem import rdDepictor\n",
        "from rdkit.Chem import PandasTools\n",
        "from IPython.display import SVG\n",
        "import ipywidgets as widgets\n",
        "import rdkit\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "AllChem.SetPreferCoordGen(True)\n",
        "from IPython.display import Image\n",
        "from openbabel import pybel\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import py3Dmol\n",
        "\n",
        "\n",
        "Type = \"smiles\" #@param [\"smiles\", \"csv\", \"sdf\"]\n",
        "\n",
        "smiles_or_filename = \"ligands.smi\" #@param {type:\"string\"}\n",
        "smiles_or_filename = os.path.join(workDir, smiles_or_filename)\n",
        "\n",
        "if Type == \"smiles\":\n",
        "  mol_list = dm.read_smi(smiles_or_filename)\n",
        "  data = dm.to_df(mol_list, smiles_column='Smiles')\n",
        "  data[\"mol\"] = data[\"Smiles\"].apply(dm.to_mol)\n",
        "  mols = data[\"mol\"].tolist()\n",
        "  mols = [dm.fix_mol(mol) for mol in mols]\n",
        "  mols = [dm.sanitize_mol(mol, sanifix=True, charge_neutral=False) for mol in mols if mol is not None]\n",
        "  mols = [dm.standardize_mol(mol, disconnect_metals=False, normalize=True, reionize=True) for mol in mols if mol is not None]\n",
        "  data[\"mol\"] = mols\n",
        "\n",
        "elif Type == \"csv\":\n",
        "  # Load the CSV file into a pandas DataFrame\n",
        "  data = pd.read_csv(smiles_or_filename)\n",
        "\n",
        "  #@markdown Column name containing the SMILES data (**.csv option only**)\n",
        "  smiles_column = 'Smiles' #@param {type:\"string\"}\n",
        "\n",
        "  # Drop rows with missing SMILES data if necessary\n",
        "  data.dropna(subset=[smiles_column], inplace=True)\n",
        "\n",
        "  # Convert the SMILES column to RDKit molecules\n",
        "  data[\"mol\"] = data[smiles_column].apply(dm.to_mol)\n",
        "\n",
        "  mols = data[\"mol\"].tolist()\n",
        "  mols = [dm.fix_mol(mol) for mol in mols]\n",
        "  mols = [dm.sanitize_mol(mol, sanifix=True, charge_neutral=False) for mol in mols if mol is not None]\n",
        "  mols = [dm.standardize_mol(mol, disconnect_metals=False, normalize=True, reionize=True) for mol in mols if mol is not None]\n",
        "  data[\"mol\"] = mols\n",
        "\n",
        "elif Type == \"sdf\":\n",
        "  mol_list = dm.read_sdf(smiles_or_filename)\n",
        "  smi_file_path = os.path.join(workDir, \"molecules.smi\")\n",
        "  smi_list = dm.to_smi(mol_list, smi_file_path)\n",
        "  data = dm.to_df(mol_list, smiles_column='Smiles')\n",
        "  data[\"mol\"] = mol_list\n",
        "  mols = data[\"mol\"].tolist()\n",
        "  mols = [dm.fix_mol(mol) for mol in mols]\n",
        "  mols = [dm.sanitize_mol(mol, sanifix=True, charge_neutral=False) for mol in mols if mol is not None]\n",
        "  mols = [dm.standardize_mol(mol, disconnect_metals=False, normalize=True, reionize=True) for mol in mols if mol is not None]\n",
        "  data[\"mol\"] = mols\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEvEXDP6KhWz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Small molecule filtering**:\n",
        "\n",
        "#@markdown Choose the type of filter you want to apply; **Options: Lipinski Rule of 5 (Ro5), Rule of 4 (Ro4), Astex Rule of 3 (Ro3), picking the centroids of clusters (Centroids) or No Filters applied**\n",
        "\n",
        "Type = \"NoFilter\" #@param [\"Ro4\", \"Ro5\", \"Ro3\", \"Centroids\", \"NoFilter\"]\n",
        "\n",
        "data['ID'] = range(1, len(data) + 1)\n",
        "\n",
        "df_descr = dm.descriptors.batch_compute_many_descriptors(mols)\n",
        "\n",
        "if Type == \"Ro4\":\n",
        "  filtered = pd.concat([data, df_descr], axis=1)\n",
        "  filtered = filtered[filtered[\"mw\"] >= 400]\n",
        "  filtered = filtered[filtered[\"n_lipinski_hba\"] >= 4]\n",
        "  filtered = filtered[filtered[\"n_rings\"] >= 4]\n",
        "  filtered = filtered[filtered[\"clogp\"] >= 4]\n",
        "  filtered\n",
        "\n",
        "elif Type == \"Ro5\":\n",
        "  filtered = pd.concat([data, df_descr], axis=1)\n",
        "  filtered = filtered[filtered[\"mw\"] <= 500]\n",
        "  filtered = filtered[filtered[\"n_lipinski_hba\"] <= 10]\n",
        "  filtered = filtered[filtered[\"n_lipinski_hbd\"] <= 5]\n",
        "  filtered = filtered[filtered[\"clogp\"] <= 5]\n",
        "  filtered\n",
        "\n",
        "elif Type == \"Ro3\":\n",
        "  filtered = pd.concat([data, df_descr], axis=1)\n",
        "  filtered = filtered[filtered[\"mw\"] <= 300]\n",
        "  filtered = filtered[filtered[\"n_lipinski_hba\"] <= 3]\n",
        "  filtered = filtered[filtered[\"n_lipinski_hbd\"] <= 3]\n",
        "  filtered = filtered[filtered[\"clogp\"] <= 3]\n",
        "  filtered = filtered[filtered[\"n_rotatable_bonds\"] <= 3]\n",
        "  filtered\n",
        "\n",
        "elif Type == \"Centroids\":\n",
        "  #@markdown Only applicable to the **Centroids** filter\n",
        "  n_centroids = \"50\" #@param {type:\"string\"}\n",
        "  cutoff_value = \"0.3\" #@param {type:\"string\"}\n",
        "  clusters, mol_clusters = dm.cluster_mols(mols, cutoff=float(cutoff_value))\n",
        "  indices, centroids = dm.pick_centroids(mols, npick=int(n_centroids), threshold=float(cutoff_value), method=\"sphere\", n_jobs=-1)\n",
        "  print(str(n_centroids) + \" centroids picked\")\n",
        "  del filtered\n",
        "\n",
        "elif Type == \"NoFilter\":\n",
        "  filtered = pd.concat([data, df_descr], axis=1)\n",
        "  filtered\n",
        "\n",
        "else:\n",
        "  pass\n",
        "\n",
        "filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4nISFJxyCjjR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Ligand Optimization with RDKit**:\n",
        "\n",
        "#@markdown Choose the output name for your optimized molecules file **(sdf format)**.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem import rdMolTransforms\n",
        "from rdkit.Chem.Draw import rdMolDraw2D\n",
        "from rdkit.Chem import rdDepictor\n",
        "from rdkit.Chem import SDWriter\n",
        "from IPython.display import SVG\n",
        "import ipywidgets as widgets\n",
        "import rdkit\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "AllChem.SetPreferCoordGen(True)\n",
        "from IPython.display import Image\n",
        "from openbabel import pybel\n",
        "import os, sys, glob\n",
        "\n",
        "\n",
        "import py3Dmol\n",
        "\n",
        "Optimized_Mol_File = \"lig_opt.sdf\" #@param {type:\"string\"}\n",
        "Optimized_Mol_File = os.path.join(workDir, Optimized_Mol_File)\n",
        "\n",
        "# SDF writer\n",
        "writer = Chem.SDWriter(Optimized_Mol_File)\n",
        "\n",
        "if 'filtered' in globals():\n",
        "# Iterating through DataFrame molecules\n",
        "  for i, (mol, mol_id) in enumerate(zip(filtered[\"mol\"], filtered[\"ID\"])):\n",
        "      print(f\"Processing molecule {i+1}/{len(filtered)}...\")\n",
        "      if mol is None:  # Check to see if molecule is valid\n",
        "          print(f\"Invalid molecule on line {i}. Ignoring...\")\n",
        "          continue\n",
        "\n",
        "    # Add hydrogens\n",
        "      hmol = Chem.AddHs(mol)\n",
        "\n",
        "    # Embedding 3D coordinates\n",
        "      status = AllChem.EmbedMolecule(hmol, maxAttempts=500, useRandomCoords=True)\n",
        "      if status != 0:\n",
        "         print(f\"Embedding failed for molecule on line {i}. Ignoring...\")\n",
        "         continue\n",
        "\n",
        "    # MMFF energy optimization\n",
        "      mp = AllChem.MMFFGetMoleculeProperties(hmol)\n",
        "      ff = AllChem.MMFFGetMoleculeForceField(hmol, mp)\n",
        "      AllChem.OptimizeMolecule(ff, maxIters=1000)\n",
        "\n",
        "    # Save temporary molecule files\n",
        "      mol_file = os.path.join(workDir, f\"{i}.mol\")\n",
        "      Chem.MolToMolFile(hmol, mol_file)\n",
        "\n",
        "\n",
        "      mol2 = Chem.MolFromMolFile(mol_file, removeHs=False)\n",
        "      if mol2 is not None:\n",
        "          mol2.SetProp(\"_Name\", f\"mol_{mol_id}\")\n",
        "          writer.write(mol2, confId=0)\n",
        "      else:\n",
        "          print(f\"Error saving/reading optimized molecule on line {i}.\")\n",
        "\n",
        "      # Clean temporary files\n",
        "      for f in glob.glob(os.path.join(workDir, \"*.mol\")):\n",
        "         os.remove(f)\n",
        "      for f in glob.glob(os.path.join(workDir, \"*.xyz\")):\n",
        "         os.remove(f)\n",
        "\n",
        "# Close the SDF writer\n",
        "  writer.close()\n",
        "\n",
        "elif 'centroids' in globals():# Assuming 'centroids' is a list or iterable containing the molecules\n",
        "  for i, mol in enumerate(centroids):\n",
        "      print(f\"Processing molecule {i+1}/{len(centroids)}...\")\n",
        "      if mol is None:  # Check if the molecule is invalid\n",
        "          print(f\"Invalid molecule on line {i}. Ignoring...\")\n",
        "          continue\n",
        "\n",
        "    # Add hydrogens\n",
        "      hmol = Chem.AddHs(mol)\n",
        "\n",
        "    # Embed 3D coordinates\n",
        "      status = AllChem.EmbedMolecule(hmol, maxAttempts=500, useRandomCoords=True)\n",
        "      if status != 0:  # Check if conformation generation failed\n",
        "          print(f\"Embedding failed for molecule on line {i}. Ignoring...\")\n",
        "          continue\n",
        "\n",
        "    # Geometry optimization with MMFF\n",
        "      mp = AllChem.MMFFGetMoleculeProperties(hmol)\n",
        "      ff = AllChem.MMFFGetMoleculeForceField(hmol, mp)\n",
        "      AllChem.OptimizeMolecule(ff, maxIters=1000)\n",
        "\n",
        "    # Save optimized molecule temporarily\n",
        "      mol_file = os.path.join(workDir, f\"{i}.mol\")\n",
        "      Chem.MolToMolFile(hmol, mol_file)\n",
        "\n",
        "    # Reload the optimized molecule to include in the SDF\n",
        "      mol2 = Chem.MolFromMolFile(mol_file, removeHs=False)\n",
        "      if mol2 is not None:\n",
        "        # Set the molecule title as \"mol_X\", where X is the sequential number\n",
        "          mol2.SetProp(\"_Name\", f\"mol_{i+1}\")  # Use i+1 to start numbering from 1\n",
        "          writer.write(mol2, confId=0)\n",
        "      else:\n",
        "          print(f\"Error saving/reading optimized molecule on line {i}.\")\n",
        "\n",
        "    # Clean up temporary files\n",
        "      for f in glob.glob(os.path.join(workDir, \"*.mol\")):\n",
        "          os.remove(f)\n",
        "      for f in glob.glob(os.path.join(workDir, \"*.xyz\")):\n",
        "          os.remove(f)\n",
        "else:\n",
        "  pass\n",
        "# Close the SDF writer\n",
        "  writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wyzlRC-sBY3J",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Parameters for the docking calculation:**\n",
        "\n",
        "#@markdown Please choose the name of the output file from the docking calculation **(do not add file extension)**:\n",
        "\n",
        "Output_file = \"output_docking_lig\" #@param {type:\"string\"}\n",
        "Output_file = os.path.join(workDir, Output_file)\n",
        "#@markdown Amount of buffer space to add the generated box (Angstroms):\n",
        "\n",
        "size = 20 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "#@markdown Exhaustiveness of the global search (roughly proportional to time):\n",
        "exhaustiveness = 10 #@param {type:\"slider\", min:2, max:64, step:2}\n",
        "\n",
        "#@markdown Explicit random seed:\n",
        "seed = \"0\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Convolutional neural network (CNN) parameter:\n",
        "\n",
        "cnn_scoring = \"rescore (default)\" #@param [\"none\", \"rescore (default)\", \"refinement\", \"all\"]\n",
        "if cnn_scoring == \"rescore (default)\":\n",
        "  cnn_scoring = \"rescore\"\n",
        "  scoring_vinardo = \" \"\n",
        "elif cnn_scoring == \"none\":\n",
        "  scoring_vinardo = \" --scoring vinardo \"\n",
        "else:\n",
        "  scoring_vinardo = \" \"\n",
        "\n",
        "#@markdown **cnn_scoring** determines at what points of the docking procedure that the CNN scoring function is used.\n",
        "\n",
        "#@markdown **none** - No CNNs used for docking. Here, uses all the empirical scoring from [Vinardo](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155183) scoring function.\n",
        "\n",
        "#@markdown **rescore** (default) - CNN used for reranking of final poses. Least computationally expensive CNN option.\n",
        "\n",
        "#@markdown **refinement** - CNN used to refine poses after Monte Carlo chains and for final ranking of output poses. 10x slower than rescore when using a GPU.\n",
        "\n",
        "#@markdown **all** - CNN used as the scoring function throughout the whole procedure. Extremely computationally intensive and not recommended.\n",
        "\n",
        "#@markdown The default CNN scoring function is an ensemble of 5 models selected to balance pose prediction performance and runtime: dense, general_default2018_3, dense_3, crossdock_default2018, and redock_default2018.\n",
        "\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "docking_output_gz = os.path.join(workDir, Output_file + \".sdf.gz\")\n",
        "docking_output = os.path.join(workDir, Output_file + \".sdf\")\n",
        "\n",
        "if os.path.exists(docking_output_gz):\n",
        "  os.remove(docking_output_gz)\n",
        "elif os.path.exists(docking_output):\n",
        "  os.remove(docking_output)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "\n",
        "if Selection == \"Pocket\":\n",
        "  gnina = \"./gnina -r \" + str(receptor) + \" -l \" +  str(Optimized_Mol_File) + \" --center_x \" + str(center_x_gnina) +  \" --center_y \" + str(center_y_gnina) +  \" --center_z \" + str(center_z_gnina) + \" --size_x \" + str(size) +  \" --size_y \" + str(size) +  \" --size_z \" + str(size) + \" --cnn_scoring \" + str(cnn_scoring) + \" --exhaustiveness \" + str(exhaustiveness) + \" -o \" + str(docking_output_gz) + str(scoring_vinardo) +  \"--num_modes 10 \" + \"--seed \" + str(int(seed))\n",
        "else:\n",
        "  gnina = \"./gnina -r \" + str(receptor) + \" -l \" +  str(Optimized_Mol_File) + \" --autobox_ligand \" + str(res_box) +  \" --autobox_add \" + str(size) + \" --cnn_scoring \" + str(cnn_scoring) + \" --exhaustiveness \" + str(exhaustiveness) + \" -o \" + str(docking_output_gz) + str(scoring_vinardo) +  \"--num_modes 10 \" + \"--seed \" + str(int(seed))\n",
        "\n",
        "zip_gnina = \"gunzip \" + str(docking_output_gz)\n",
        "\n",
        "original_stdout = sys.stdout # Save a reference to the original standard output\n",
        "with open('gnina.sh', 'w') as f:\n",
        "    sys.stdout = f # Change the standard output to the file we created.\n",
        "    print(gnina)\n",
        "    print(zip_gnina)\n",
        "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
        "\n",
        "!chmod 700 gnina.sh 2>&1 1>/dev/null\n",
        "!bash gnina.sh\n",
        "\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6KLx8O2J4kES"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from rdkit.Chem import rdFMCS,AllChem, Draw, PandasTools\n",
        "import seaborn as sns\n",
        "from openbabel import pybel\n",
        "\n",
        "#@title **Docking Analysis:**\n",
        "\n",
        "#@markdown Please choose which parameter will be used to sort the results **(CNN_VS is recommended for Virtual Screening studies)**:\n",
        "Parameter = \"CNN_VS\" #@param [\"minimizedAffinity\", \"CNNscore\", \"CNN_VS\", \"CNNaffinity\"]\n",
        "\n",
        "#@markdown Please fill in the blanks with the sdf file name from the docking output:\n",
        "\n",
        "# Input file path\n",
        "Docking_output = \"output_docking_lig.sdf\" #@param {type:\"string\"}\n",
        "Docking_output = os.path.join(workDir, Docking_output)\n",
        "\n",
        "# Read the entire content of the file\n",
        "with open(Docking_output, \"r\") as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Process the lines to update the titles\n",
        "molecule_count = 0  # Counter for the current molecule (e.g., mol_1, mol_2)\n",
        "solution_count = 0  # Counter for the solution number (e.g., _1, _2, ..., _10)\n",
        "current_molecule = None  # Stores the current molecule title (e.g., mol_1)\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    # Check if the line contains a molecule title (e.g., mol_1)\n",
        "    if line.startswith(\"mol_\"):\n",
        "        # Extract the molecule number (e.g., 1 from mol_1)\n",
        "        molecule_number = line.strip().split(\"_\")[1]\n",
        "\n",
        "        # If this is a new molecule, reset the solution counter\n",
        "        if molecule_number != current_molecule:\n",
        "            current_molecule = molecule_number\n",
        "            solution_count = 0\n",
        "\n",
        "        # Increment the solution counter\n",
        "        solution_count += 1\n",
        "\n",
        "        # Create the new title (e.g., mol_1_1, mol_1_2, etc.)\n",
        "        new_title = f\"mol_{molecule_number}_{solution_count}\\n\"\n",
        "\n",
        "        # Update the line in the list\n",
        "        lines[i] = new_title\n",
        "\n",
        "# Write the updated content back to the input file\n",
        "with open(Docking_output, \"w\") as outfile:\n",
        "    outfile.writelines(lines)\n",
        "\n",
        "# Load and process docking results\n",
        "VinaPoses=PandasTools.LoadSDF(Docking_output)\n",
        "AllPoses=pd.concat([VinaPoses])\n",
        "\n",
        "# List to store scores and titles\n",
        "scores = []\n",
        "\n",
        "# Read the SDF file\n",
        "for mol in pybel.readfile('sdf', Docking_output):\n",
        "    molecule_title = mol.title.strip()\n",
        "\n",
        "    # Create a dictionary to store the scores\n",
        "    score_data = {\n",
        "        'Molecule_Solution': molecule_title,\n",
        "        'minimizedAffinity': float(mol.data['minimizedAffinity']),\n",
        "        'CNNscore': float(mol.data['CNNscore']),\n",
        "        'CNNaffinity': float(mol.data['CNNaffinity']),\n",
        "        'CNN_VS': float(mol.data['CNN_VS'])\n",
        "    }\n",
        "\n",
        "    scores.append(score_data)\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "scores_df = pd.DataFrame(scores)\n",
        "\n",
        "# Reorder columns\n",
        "scores_df = scores_df[['Molecule_Solution', 'minimizedAffinity', 'CNNscore', 'CNNaffinity', 'CNN_VS']]\n",
        "\n",
        "# Sort the DataFrame based on the selected parameter\n",
        "if Parameter == \"minimizedAffinity\":\n",
        "    scores_sorted = scores_df.sort_values(by=Parameter, ascending=True).reset_index(drop=True)\n",
        "else:\n",
        "    scores_sorted = scores_df.sort_values(by=Parameter, ascending=False).reset_index(drop=True)\n",
        "\n",
        "scores_sorted.to_csv(os.path.join(workDir, Parameter + \"_sorted.csv\"), index=False)\n",
        "#scores_sorted\n",
        "\n",
        "# New code to write the sorted molecules to a new SDF file\n",
        "# Dictionary to store molecules by their titles\n",
        "molecule_dict = {}\n",
        "\n",
        "# Read the SDF file again and store molecules in the dictionary\n",
        "for mol in pybel.readfile('sdf', Docking_output):\n",
        "    molecule_title = mol.title.strip()\n",
        "    molecule_dict[molecule_title] = mol\n",
        "\n",
        "# Write the molecules to a new SDF file in the order specified by the sorted DataFrame\n",
        "#@markdown Please choose the name of the output sdf file sorted according to the Parameter selected:\n",
        "Output_sdf = \"lig_sorted_CNN_VS.sdf\" #@param {type:\"string\"}\n",
        "Output_sdf = os.path.join(workDir, Output_sdf)\n",
        "\n",
        "with open(Output_sdf, 'w') as outfile:\n",
        "    for molecule_solution in scores_sorted['Molecule_Solution']:\n",
        "        if molecule_solution in molecule_dict:\n",
        "            mol = molecule_dict[molecule_solution]\n",
        "            outfile.write(mol.write(format='sdf'))\n",
        "        else:\n",
        "            print(f\"Warning: Molecule '{molecule_solution}' not found in the SDF file.\")\n",
        "\n",
        "print(f\"Sorted SDF file saved to {Output_sdf}\")\n",
        "scores_sorted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Filter for the top 20 molecules in the output sdf file (preparation for PLACER step)**\n",
        "#@markdown This step filters the sorted SDF file to keep only the top solutions from the docking step. The number of molecules can be changed by modifying the value of the **num_molecules** variable in the code.\n",
        "\n",
        "from rdkit import Chem\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "sdf_file = Output_sdf\n",
        "\n",
        "def filter_top_molecules(sdf_file, num_molecules=20):\n",
        "    \"\"\"Filters the top N molecules from an SDF file, keeping original titles.\n",
        "\n",
        "    Args:\n",
        "        sdf_file: Path to the SDF file.\n",
        "        num_molecules: Number of top molecules to extract (default is 20).\n",
        "\n",
        "    Returns:\n",
        "        A list of RDKit Mol objects representing the top molecules.\n",
        "        Returns an empty list if the file does not exist or if an error occurs.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(sdf_file):\n",
        "        print(f\"Error: File not found - {sdf_file}\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        suppl = Chem.SDMolSupplier(sdf_file)\n",
        "        top_molecules = []\n",
        "        for i, mol in enumerate(suppl):\n",
        "            if mol is not None:  # Check for valid molecules\n",
        "                top_molecules.append(mol)\n",
        "                if i + 1 == num_molecules:\n",
        "                    break  # Stop after extracting the desired number\n",
        "\n",
        "        return top_molecules\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "top_molecules = filter_top_molecules(Output_sdf)  # Using Output_sdf\n",
        "\n",
        "if top_molecules:\n",
        "    print(f\"Successfully extracted {len(top_molecules)} molecules.\")\n",
        "\n",
        "    writer = Chem.SDWriter(os.path.join(workDir, \"top_20_molecules.sdf\"))\n",
        "    for mol in top_molecules:\n",
        "        writer.write(mol)\n",
        "    writer.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0f8yPvbPSMnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Ay3nTpSwWm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Convert molecules to PDB files (preparation for PLACER step)**\n",
        "\n",
        "\n",
        "import os\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import PandasTools\n",
        "from rdkit.Chem import AllChem # Added import statement\n",
        "\n",
        "def sdf_to_individual_pdbs(sdf_file, output_dir):\n",
        "    \"\"\"\n",
        "    Convert each molecule in an SDF file to individual PDB files.\n",
        "\n",
        "    Args:\n",
        "        sdf_file (str): Path to the input SDF file\n",
        "        output_dir (str): Directory to save PDB files\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read the SDF file\n",
        "    supplier = Chem.SDMolSupplier(sdf_file)\n",
        "\n",
        "    for i, mol in enumerate(supplier):\n",
        "        if mol is not None:\n",
        "            # Get the molecule title (first line of SDF record)\n",
        "            title = mol.GetProp(\"_Name\") if mol.HasProp(\"_Name\") else f\"molecule_{i+1}\"\n",
        "\n",
        "            # Clean the title to make it filesystem-safe\n",
        "            safe_title = \"\".join(c if c.isalnum() or c in \"_- \" else \"_\" for c in title)\n",
        "            safe_title = safe_title.strip()\n",
        "\n",
        "            # Generate output path\n",
        "            pdb_file = os.path.join(output_dir, f\"{safe_title}.pdb\")\n",
        "            hmol = Chem.AddHs(mol, addCoords=True)\n",
        "            # Write to PDB file\n",
        "            Chem.MolToPDBFile(hmol, pdb_file)\n",
        "\n",
        "            print(f\"Saved: {pdb_file}\")\n",
        "        else:\n",
        "            print(f\"Warning: Failed to process molecule {i+1}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) < 3:\n",
        "        print(\"Usage: python sdf_to_pdbs.py <input.sdf> <output_directory>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    sdf_file = \"lig_sorted_CNN_VS.sdf\" #@param {type:\"string\"}\n",
        "    sdf_file = os.path.join(workDir, sdf_file)\n",
        "    output_dir = os.path.join(workDir, 'conformer_docking')\n",
        "\n",
        "    sdf_to_individual_pdbs(sdf_file, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "colXLYLVqlD6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Convert single SDF to multiple SDF files (preparation for PLACER step)**\n",
        "\n",
        "\n",
        "import os\n",
        "from rdkit import Chem\n",
        "\n",
        "def split_sdf_to_individual_files(input_sdf, output_dir):\n",
        "    \"\"\"\n",
        "    Split an SDF file into individual molecule files.\n",
        "\n",
        "    Args:\n",
        "        input_sdf (str): Path to input SDF file\n",
        "        output_dir (str): Directory to save individual molecule files\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read the SDF file\n",
        "    supplier = Chem.SDMolSupplier(input_sdf)\n",
        "\n",
        "    for i, mol in enumerate(supplier):\n",
        "        if mol is not None:\n",
        "            # Get molecule title (or create one if none exists)\n",
        "            if mol.HasProp(\"_Name\"):\n",
        "                title = mol.GetProp(\"_Name\")\n",
        "            else:\n",
        "                title = f\"molecule_{i+1}\"\n",
        "\n",
        "            # Clean the title to make it filesystem-safe\n",
        "            safe_title = \"\".join(c if c.isalnum() or c in \"_-\" else \"_\" for c in title)\n",
        "            safe_title = safe_title.strip()\n",
        "\n",
        "            # Generate output path\n",
        "            output_file = os.path.join(output_dir, f\"{safe_title}.sdf\")\n",
        "\n",
        "            hmol = Chem.AddHs(mol, addCoords=True)\n",
        "            # Write molecule to individual SDF file\n",
        "            writer = Chem.SDWriter(output_file)\n",
        "            writer.write(hmol)\n",
        "            writer.close()\n",
        "\n",
        "            print(f\"Saved: {output_file}\")\n",
        "        else:\n",
        "            print(f\"Warning: Failed to process molecule {i+1}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "\n",
        "    if len(sys.argv) != 3:\n",
        "        print(\"Usage: python split_sdf.py <input.sdf> <output_directory>\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    input_sdf = \"lig_sorted_CNN_VS.sdf\" #@param {type:\"string\"}\n",
        "    input_sdf = os.path.join(workDir, input_sdf)\n",
        "    output_dir = output_dir\n",
        "\n",
        "    split_sdf_to_individual_files(input_sdf, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUeq7W3ao74e",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "#@title **Calculating the protein-small molecule conformational ensembles with PLACER**\n",
        "# Activate the conda environment\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate placer_env\n",
        "\n",
        "# Run the Python script\n",
        "python <<EOF\n",
        "import sys, os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "Google_Drive_Path = '/content/drive/MyDrive/' #@param {type:\"string\"}\n",
        "workDir = Google_Drive_Path\n",
        "output_dir = os.path.join(workDir, 'conformer_docking')\n",
        "#@markdown Number of samples to generate, 50-100 is a good number in most cases.\n",
        "n_samples = 50 #@param {type:\"slider\", min:10, max:200, step:10}\n",
        "\n",
        "# Get all molecule files in the output directory\n",
        "mol_files = glob.glob(os.path.join(output_dir, \"mol_*.pdb\"))\n",
        "\n",
        "# Process each molecule\n",
        "for mol_file in mol_files:\n",
        "    # Extract the mode_number from filename (e.g., \"mol_253_3.pdb\" -> \"253_3\")\n",
        "    base_name = os.path.basename(mol_file)\n",
        "    mode_number = base_name.replace(\"mol_\", \"\").replace(\".pdb\", \"\")\n",
        "\n",
        "    receptor_file = os.path.join(workDir, \"receptor.pdb\")\n",
        "    output_file = os.path.join(output_dir, f\"pose_{mode_number}.pdb\")  # Save in output_dir\n",
        "\n",
        "    # Combine receptor and ligand into a single PDB file\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        with open(receptor_file, 'r') as infile1:\n",
        "            lines = infile1.readlines()\n",
        "            outfile.writelines(lines[:-1])  # Write all lines except the last\n",
        "        with open(mol_file, 'r') as infile2:\n",
        "            outfile.write(infile2.read())\n",
        "\n",
        "    receptor = output_file\n",
        "    ligand = os.path.join(output_dir, f\"mol_{mode_number}.sdf\")  # Corresponding SDF file\n",
        "\n",
        "    # Check if ligand file exists\n",
        "    if not os.path.exists(ligand):\n",
        "        print(f\"Warning: Ligand file {ligand} not found, skipping {mol_file}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing molecule {mode_number}...\")\n",
        "\n",
        "    # PLACER processing\n",
        "    import json\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    sys.path.append(f\"/content/PLACER/\")\n",
        "    import PLACER\n",
        "\n",
        "    # Initializing PLACER model with default checkpoint\n",
        "    placer = PLACER.PLACER()\n",
        "\n",
        "    print(f\"\"\"\n",
        "    ###############################################\n",
        "    # Predicting for mode {mode_number} #\n",
        "    ###############################################\n",
        "    \"\"\")\n",
        "\n",
        "    pdbfile = receptor\n",
        "    pdbstr = open(pdbfile, \"r\").read()\n",
        "\n",
        "    ODIR = os.path.join(output_dir, \"outputs_PLACER\")  # Save outputs in output_dir\n",
        "\n",
        "    inp_dict = {\"ligand_reference\": {\"UNL\": ligand},\n",
        "                \"name\": os.path.basename(pdbfile).replace(\".pdb\", \"\"),\n",
        "                \"pdb\": pdbstr}\n",
        "\n",
        "    pl_inp = PLACER.PLACERinput()\n",
        "    pl_inp.create_from_dict(inp_dict)\n",
        "    outputs_denovo2 = placer.run(pl_inp, int(n_samples))\n",
        "\n",
        "    # Ranking outputs by prmsd\n",
        "    outputs_denovo2 = PLACER.utils.rank_outputs(outputs_denovo2, \"prmsd\")\n",
        "\n",
        "    # Dumping output models to PDB\n",
        "    os.makedirs(ODIR, exist_ok=True)\n",
        "    print(f\"Writing outputs to {ODIR}\")\n",
        "    PLACER.protocol.dump_output(outputs_denovo2, f\"{ODIR}/{pl_inp.name()}\")\n",
        "\n",
        "    df = pd.DataFrame.from_dict({k: [outputs_denovo2[n][k] for n in outputs_denovo2] for k in outputs_denovo2[0].keys() if k not in [\"item\", \"model\", \"center\"]})\n",
        "\n",
        "    print(f\"Completed processing for mode {mode_number}\")\n",
        "\n",
        "print(\"\\nAll molecules processed successfully!\")\n",
        "EOF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Batch Filter PLACER Ensembles**\n",
        "\n",
        "#@markdown **Filter unphysical samples (e.g., steric clashes) from the PLACER ensemble**\n",
        "\n",
        "#@markdown This cell applies structural filters to ligands in an ensemble to remove unphysical conformations while retaining the full system for valid frames.\n",
        "\n",
        "#@markdown The ligand is evaluated on each frame using the following criteria:\n",
        "\n",
        "#@markdown - â **Steric Clashes** â Checks if atoms are too close together.\n",
        "#@markdown - â **Bond Lengths** â Identifies abnormally long or short bonds.\n",
        "#@markdown - â **Bond Angles** â Flags angles outside a reasonable range.\n",
        "\n",
        "#@markdown Frames where the ligand passes all filters are saved into a new trajectory file, preserving the entire molecular system.\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import align\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "Google_Drive_Path = '/content/drive/MyDrive/' #@param {type:\"string\"}\n",
        "workDir = Google_Drive_Path\n",
        "output_dir = os.path.join(workDir, 'conformer_docking')\n",
        "\n",
        "def process_placer_ensemble(pdb_path, output_dir):\n",
        "    \"\"\"Process a single PLACER ensemble\"\"\"\n",
        "    try:\n",
        "        u = mda.Universe(pdb_path, pdb_path)\n",
        "\n",
        "        # Align protein structure\n",
        "        average = align.AverageStructure(u, u, select='protein and name CA', ref_frame=0).run()\n",
        "        ref = average.results.universe\n",
        "        aligner = align.AlignTraj(u, ref, select='protein and name CA', in_memory=True).run()\n",
        "\n",
        "        # Save aligned topology\n",
        "        pdb_file = u.select_atoms(\"all\")\n",
        "        topology_path = os.path.join(output_dir, \"temp_topology.pdb\")\n",
        "        pdb_file.write(topology_path)\n",
        "\n",
        "        # Reload with proper topology\n",
        "        u = mda.Universe(topology_path, pdb_path)\n",
        "        all_atoms = u.select_atoms(\"all\")\n",
        "\n",
        "        # Extract mode_number from filename\n",
        "        base_name = os.path.basename(pdb_path)\n",
        "        mode_number = base_name.split('_model.pdb')[0].split('pose_')[-1]\n",
        "\n",
        "        # Define output path\n",
        "        # new_traj = os.path.join(output_dir, f\"Filter_PLACER/PLACER_filter.pdb\")\n",
        "        new_traj_dir = os.path.join(output_dir, \"Filter_PLACER\")\n",
        "        os.makedirs(new_traj_dir, exist_ok=True)\n",
        "        new_traj = os.path.join(new_traj_dir, f\"PLACER_filter_{mode_number}.pdb\")\n",
        "\n",
        "\n",
        "        with mda.Writer(new_traj, all_atoms.n_atoms) as W:\n",
        "            for ts in u.trajectory:\n",
        "                ligand_sel = get_ligand_selection(u)\n",
        "                if ligand_is_realistic(ligand_sel):\n",
        "                    W.write(all_atoms)\n",
        "\n",
        "        print(f\"â Processed {mode_number}: Saved to {new_traj}\")\n",
        "        os.remove(topology_path)  # Clean up temporary file\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"â Failed to process {pdb_path}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Your existing filter functions (keep these exactly the same)\n",
        "def get_ligand_selection(universe):\n",
        "    ligand = universe.select_atoms(\"resname UNL\")\n",
        "    if len(ligand) == 0:\n",
        "        raise ValueError(\"Ligand selection is empty. Check if 'resname UNL' is correct.\")\n",
        "    ligand.guess_bonds()\n",
        "    return ligand\n",
        "\n",
        "def has_steric_clashes(ligand, min_distance=1.0):\n",
        "    \"\"\"Check if ligand atoms are too close together (steric clashes).\"\"\"\n",
        "    if len(ligand) < 2:\n",
        "        return False\n",
        "    distances = pdist(ligand.positions)\n",
        "    return any(dist < min_distance for dist in distances)\n",
        "\n",
        "def has_unusual_bond_lengths(ligand, min_bond=0.9, max_bond=1.8):\n",
        "    \"\"\"Check for unrealistic bond lengths in the ligand.\"\"\"\n",
        "    if len(ligand.bonds) == 0:\n",
        "        return False\n",
        "    for bond in ligand.bonds.to_indices():\n",
        "        if bond[0] >= len(ligand) or bond[1] >= len(ligand):\n",
        "            continue\n",
        "        dist = np.linalg.norm(ligand.positions[bond[0]] - ligand.positions[bond[1]])\n",
        "        if dist < min_bond or dist > max_bond:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def has_unusual_angles(ligand, min_angle=80, max_angle=180):\n",
        "    \"\"\"Check for unrealistic bond angles.\"\"\"\n",
        "    if not hasattr(ligand, \"angles\") or len(ligand.angles) == 0:\n",
        "        return False\n",
        "    positions = ligand.positions\n",
        "    for angle in ligand.angles.to_indices():\n",
        "        if max(angle) >= len(positions):\n",
        "            continue\n",
        "        a, b, c = positions[angle[0]], positions[angle[1]], positions[angle[2]]\n",
        "        v1 = a - b\n",
        "        v2 = c - b\n",
        "        cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "        theta = np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
        "        if theta < min_angle or theta > max_angle:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def ligand_is_realistic(ligand):\n",
        "    return not (\n",
        "        has_steric_clashes(ligand) or\n",
        "        has_unusual_bond_lengths(ligand) or\n",
        "        has_unusual_angles(ligand)\n",
        "    )\n",
        "\n",
        "# Main processing loop\n",
        "placer_outputs = glob.glob(os.path.join(output_dir, \"outputs_PLACER/pose_*_model.pdb\"))\n",
        "\n",
        "if not placer_outputs:\n",
        "    print(\"â No PLACER outputs found in:\", os.path.join(output_dir, \"outputs_PLACER\"))\n",
        "else:\n",
        "    print(f\"ð Found {len(placer_outputs)} PLACER outputs to process\")\n",
        "    for pdb_path in placer_outputs:\n",
        "        process_placer_ensemble(pdb_path, output_dir)\n",
        "    print(\"â All processing complete!\")"
      ],
      "metadata": {
        "id": "-iZWMWxHZx5p",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Batch Calculate Binding Affinity for All PLACER Ensembles**\n",
        "\n",
        "#!mamba install -c conda-forge pymol-open-source -y\n",
        "#!pip install qcelemental\n",
        "#!pip install torch-geometric\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import MDAnalysis as mda\n",
        "from MDAnalysis.analysis import rms, align\n",
        "import numpy as np\n",
        "from pdbfixer import PDBFixer\n",
        "from pymol import cmd\n",
        "import glob\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "from openmm.app import PDBFile\n",
        "import qcelemental as qcel\n",
        "\n",
        "Add_hydrogens = True #@param {type:\"boolean\"}\n",
        "\n",
        "Skip = 1 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "Google_Drive_Path = '/content/drive/MyDrive/' #@param {type:\"string\"}\n",
        "workDir = Google_Drive_Path\n",
        "output_dir = os.path.join(workDir, 'conformer_docking') # Define output_dir here\n",
        "\n",
        "# Initialize results storage\n",
        "all_results = []\n",
        "\n",
        "# Find all filtered PLACER outputs\n",
        "placer_outputs = glob.glob(os.path.join(output_dir, \"Filter_PLACER/PLACER_filter_*.pdb\")) # Corrected path\n",
        "\n",
        "if not placer_outputs:\n",
        "    print(\"â No PLACER outputs found in:\", os.path.join(output_dir, \"Filter_PLACER\"))\n",
        "else:\n",
        "    print(f\"ð Found {len(placer_outputs)} PLACER outputs to process\")\n",
        "\n",
        "    for filtered_pdb in placer_outputs:\n",
        "        cmd.delete(\"all\")\n",
        "        # Extract mode_number from filename\n",
        "        base_name = os.path.basename(filtered_pdb)\n",
        "        mode_number = base_name.split('PLACER_filter_')[-1].split('.pdb')[0] # Corrected mode_number extraction\n",
        "\n",
        "        print(f\"\\nProcessing ensemble {mode_number}...\")\n",
        "\n",
        "        # Create unique directory for this mode\n",
        "        pdb_dir = os.path.join(output_dir, f'PDBs_affinity_{mode_number}')\n",
        "        os.makedirs(pdb_dir, exist_ok=True)\n",
        "\n",
        "        # Clear existing files\n",
        "        if os.path.exists('/content/input_ensemble.csv'):\n",
        "            os.remove('/content/input_ensemble.csv')\n",
        "\n",
        "        # Load the trajectory - Use the filtered PDB for both topology and trajectory\n",
        "        u = mda.Universe(filtered_pdb, filtered_pdb)\n",
        "\n",
        "\n",
        "        # Alignment and processing (same as original)\n",
        "        average = align.AverageStructure(u, u, select='protein and name CA', ref_frame=0).run()\n",
        "        ref = average.results.universe\n",
        "        aligner = align.AlignTraj(u, ref, select='protein and name CA', in_memory=True).run()\n",
        "\n",
        "        pdb_file = u.select_atoms(\"all\")\n",
        "        topology_path = os.path.join(pdb_dir, f\"topology_{mode_number}.pdb\") # Save topology in pdb_dir\n",
        "        pdb_file.write(topology_path)\n",
        "\n",
        "        u1 = mda.Universe(topology_path, filtered_pdb)\n",
        "\n",
        "        # Prepare CSV file\n",
        "        csv_file_path = Path('/content/input_ensemble.csv')\n",
        "        with open(csv_file_path, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['unique_id', 'sdf_file', 'pdb_file'])\n",
        "\n",
        "        # Process frames\n",
        "        protein_atoms = u1.select_atoms(\"protein\")\n",
        "        ligand_atoms = u1.select_atoms(\"resname UNL\")\n",
        "        i = 0\n",
        "\n",
        "        for ts in u1.trajectory[0:len(u1.trajectory):int(Skip)]:\n",
        "            i += 1\n",
        "            try:\n",
        "                # Write protein and ligand frames\n",
        "                with mda.Writer(os.path.join(pdb_dir, f'receptor_frame{i}.pdb'), protein_atoms.n_atoms) as W:\n",
        "                    W.write(protein_atoms)\n",
        "                with mda.Writer(os.path.join(pdb_dir, f'ligand_frame{i}.pdb'), ligand_atoms.n_atoms) as W:\n",
        "                    W.write(ligand_atoms)\n",
        "\n",
        "                # Convert to SDF and add hydrogens\n",
        "                name_pymol = f'ligand_frame{i}'\n",
        "                pdb_file = os.path.join(pdb_dir, f'ligand_frame{i}.pdb')\n",
        "                sdf_file = os.path.join(pdb_dir, f'ligand_frame{i}.sdf')\n",
        "\n",
        "                if Add_hydrogens:\n",
        "                    # Protein hydrogens\n",
        "                    fixer = PDBFixer(filename=os.path.join(pdb_dir, f'receptor_frame{i}.pdb'))\n",
        "                    fixer.removeHeterogens()\n",
        "                    fixer.findMissingResidues()\n",
        "                    fixer.findMissingAtoms()\n",
        "                    fixer.addMissingAtoms()\n",
        "                    fixer.addMissingHydrogens(pH=7.4)\n",
        "                    receptor_pdb = os.path.join(pdb_dir, f'receptor_frame{i}_H.pdb')\n",
        "                    PDBFile.writeFile(fixer.topology, fixer.positions, open(receptor_pdb, 'w'))\n",
        "\n",
        "                    # Ligand hydrogens\n",
        "                    cmd.delete(\"all\")\n",
        "                    cmd.load(pdb_file, name_pymol)\n",
        "                    cmd.h_add(name_pymol)\n",
        "                    cmd.save(sdf_file, name_pymol, format=\"sdf\")\n",
        "                    cmd.delete(\"all\")\n",
        "                else:\n",
        "                    cmd.delete(\"all\")\n",
        "                    receptor_pdb = os.path.join(pdb_dir, f'receptor_frame{i}.pdb')\n",
        "                    cmd.load(pdb_file, name_pymol)\n",
        "                    cmd.save(sdf_file, name_pymol, format=\"sdf\")\n",
        "                    cmd.delete(\"all\")\n",
        "\n",
        "                # Record in CSV\n",
        "                with open(csv_file_path, mode='a', newline='') as file:\n",
        "                    writer = csv.writer(file)\n",
        "                    writer.writerow([i, sdf_file, receptor_pdb])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"â ï¸ Error processing frame {i} of {mode_number}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Run AEV-PLIG prediction\n",
        "        result = subprocess.run(\n",
        "            f\"python /content/AEV-PLIG/process_and_predict.py --dataset_csv=/content/input_ensemble.csv --data_name=ensemble_{mode_number} --trained_model_name=model_GATv2Net_ligsim90_fep_benchmark\",\n",
        "            shell=True,\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode != 0:\n",
        "            print(f\"â AEV-PLIG failed for {mode_number}: {result.stderr}\")\n",
        "            continue\n",
        "\n",
        "        # Process predictions\n",
        "        csv_file_path = Path(f'/content/AEV-PLIG/output/predictions/ensemble_{mode_number}_predictions.csv')\n",
        "\n",
        "        if not csv_file_path.exists():\n",
        "            print(f\"â ï¸ No predictions file found for {mode_number}\")\n",
        "            continue\n",
        "\n",
        "        dg_kcal_list = []\n",
        "        with open(csv_file_path, mode='r', newline='') as file:\n",
        "            csv_reader = csv.DictReader(file)\n",
        "            for row in csv_reader:\n",
        "                preds_values = float(row['preds'])\n",
        "                r = 8.3145  # J/mol/K\n",
        "                t = 297  # K\n",
        "                k = 10**-preds_values\n",
        "                dg_j = r * t * np.log(k)\n",
        "                dg_kcal = dg_j / 4184\n",
        "                dg_kcal_list.append(dg_kcal)\n",
        "\n",
        "        # Store results\n",
        "        dg_average = np.mean(dg_kcal_list)\n",
        "        dg_std = np.std(dg_kcal_list)\n",
        "        all_results.append({\n",
        "            'mode_number': mode_number,\n",
        "            'average_affinity': dg_average,\n",
        "            'std_dev': dg_std,\n",
        "            'num_frames': len(dg_kcal_list)\n",
        "        })\n",
        "\n",
        "        print(f\"â Processed {mode_number}: Avg affinity = {dg_average:.4f} Â± {dg_std:.4f} kcal/mol\")\n",
        "\n",
        "        # Save individual results\n",
        "        df = pd.read_csv(csv_file_path)\n",
        "        frames = df['unique_id']\n",
        "        new_df = pd.DataFrame({\n",
        "            'Frames': frames,\n",
        "            'Binding Affinity (kcal/mol)': dg_kcal_list\n",
        "        })\n",
        "        output_csv = os.path.join(workDir, f'Affinity_AEV-PLIG_PLACER_{mode_number}.csv')\n",
        "        new_df.to_csv(output_csv, index=False)\n",
        "\n",
        "        # Plot individual results\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        sc = plt.scatter(frames, dg_kcal_list, c=dg_kcal_list, cmap='viridis', s=50)\n",
        "        cbar = plt.colorbar(sc)\n",
        "        cbar.set_label('Binding Affinity (kcal/mol)', fontsize=14, fontweight='bold')\n",
        "        plt.axhline(dg_average, color='red', linestyle='--', label='Average')\n",
        "        plt.ylim(np.min(dg_kcal_list)-1, np.max(dg_kcal_list)+1)\n",
        "        plt.xlabel('Frames', fontsize=12, fontweight='bold')\n",
        "        plt.ylabel('Binding Affinity (kcal/mol)', fontsize=12, fontweight='bold')\n",
        "        plt.title(f'Binding Affinity for Mode {mode_number}', fontsize=12, fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "        plot_path = os.path.join(workDir, f\"Affinity_AEV-PLIG_PLACER_{mode_number}.png\")\n",
        "        plt.savefig(plot_path, dpi=600, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Clean up\n",
        "        cmd.delete(\"all\")\n",
        "\n",
        "    # Save summary of all results\n",
        "    summary_df = pd.DataFrame(all_results)\n",
        "    summary_csv = os.path.join(workDir, 'AEV-PLIG_Summary_Results.csv')\n",
        "    summary_df.to_csv(summary_csv, index=False)\n",
        "\n",
        "    print(\"\\nâ All processing complete! Summary saved to:\", summary_csv)"
      ],
      "metadata": {
        "id": "KUcKWXVgPUXg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}